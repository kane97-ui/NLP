# Word2vec

## 标准word2vec

**条件概率**
$$
p(w_0|w_c)={exp(u_0^Tv_c)\over \sum_{i\in V}exp(u_i^Tv_c)}
$$
**该条件概率对应的对数损失**
$$
logP(w_0|w_c)=u_0^Tv_c-log(\sum_{i\in V}exp(u_i^Tv_c))
$$
**损失函数**
$$
loss=-log\prod_{k=1}^N p(w_k|w_c)
$$
**由于softmax运算考虑了背景词可能是词典V中的任一词，以上损失包含了词典大小数目的累加。softmax其实是已经考虑了负样本问题，而且这些负样本是除背景词外词典里的所有词**

## 负采样/Negative Sampling

为了减小softmax的计算量，去负采样。设背景词 $w_0$ 出现在中心词$w_c$的一个背景窗口为事件P，我们根据分布$p(w)$采样K个未出现在该背景窗口中的词。然后将softmax改为二分，每个中心词对应到词典中的每个词看成二项分布。

**所以每个词的条件概率公式为**
$$
\sigma(x)={1\over 1+exp(-x)}\\
p(w_{t+j}=y|w_t)=\sigma(f(w_t))^y. (1-\sigma(f(w_t))^{1-y}\\
p(w_{t+j}|w_t)=p(w_{t+j}=1|w_t)\prod_{k=1}^{K} p(w_{k}=0|w_t)
$$
设文本序列中的时间步t的词为$w_t$在词典中的索引为$i_t$，噪声词$w_k$ 在词典中的索引为$h_k$ 

 **损失函数**
$$
loss=-logP(w_{t+j}|w_t)=-logP(w_{t+j}=1|w_t)-\sum_{k=1}^K logP(w_k=0|w_k)\\
=-log\sigma(u^T_{h_{t+j}}v_{i_t})-\sum_{k=1}^K logP(1-\sigma(u^T_{h_{k}}v_{i_t})
$$

### 总结

* 针对softmax运算导致的每次梯度计算开销过大·，将softmax调整为sogmoid函数，当然对应的含义也由给定中心词，每个词作为背景词的概率，变成了给定了中心词，每个词出现在背景窗口中的概率
* 进行负采样，引入负采样，负采样的名字就是取了第二个改进点。



## 层次softmax

**改进**：为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。**更新的就是一条路径上的权重：从输入的根节点到对应的叶子结点**

* 树模型：计算量从$O(n)$到$O(logn)$ 
* 由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。

### Huffman 编码

* 在编码中希望频率高的编码长度短，频率低的编码长度长，以优化整个报文编码
* 一个字符的编码不能是另一个字符编码的前缀
* Huffman编码即能满足前缀编码的条件，又能保证报文编码总长最短。
* huffman二叉树的带权路径最小，其中权值越大的节点越接近根节点。值越小的节点越远离根节点。

### Huffman树的构造

```c
while (单词列表长度>1) {
    从单词列表中挑选出出现频率最小的两个单词 ;
    创建一个新的中间节点，其左右节点分别是之前的两个单词节点 ;
    从单词列表中删除那两个单词节点并插入新的中间节点 ;
}
```

<img src="/Users/kanghaoyu/Library/Mobile Documents/com~apple~CloudDocs/上岸宝典/NLP/huffman.png" alt="huffman" style="zoom:50%;" />

约定：

- 权值大的作为左节点，权值小的作为右节点
- 左孩子编码为1，右孩子编码为0

**更新的就是一条路径上的权重：从输入到对应的叶子结点**

我们使用最大似然法来寻找所有节点的词向量和所有内部节点$\theta$

设对应的huffman编码为：$d_i^w\in{0,1}$

则某一个节点的的逻辑回归概率为
$$
p(d_j^w|x_w,\theta_{j-1}^w)=\sigma(x^T_w\theta^w_{j-1})^{1-d_j^w}[1-\sigma(x_w^T\theta^w_{j-1})]
$$
损失函数
$$
Loss==log\sum_{j=2}^{l_w}((1-d_j^w)log\sigma(x^T_w\theta^w_{j-1})+d_j^wlog\sigma(1-x^T_w\theta^w_{j-1}))
$$


## 简单讲一下word2vec

1. 首次word2vec是无监督学习，它的目的是将word向量从one-hot的高维稀疏向量嵌入到低位的空间当中。
2. 它主要的思想是，处在相似语境下或者说周围单词相似的的单词的embedding具有很高的相似度。
3. 从而word2vec分为两种，第一种是skip-gram，第二种是cbow。skip-gram是通过中心词去预测周围词，而cbow是通过周围词去预测中心词。
4. 那网络结构的话，word2vec相当于一个三层的感知机。
5. 输入和输出都是单词的one-hot向量，隐藏层则是单词的embedding表征
6. 损失函数是用的softmax后的logloss。
7. 但是因为词典非常大，那每次计算softmax会用到整个词典的单词，所有计算量是很大的，所以有以下两个改进：
   * 负采样，负采样通过随机采取k个未出现在中心词滑动窗口内的词作为负样本，然后将softmax改为sigmoid，将每个输出看成二项分布。为1表示输出是背景词，0表示不是背景词。第二个改进
   * 层次softmax：用霍夫曼树来代替从隐藏层到输出softmax层的映射。**更新的就是一条路径上的权重：从输入的根节点到对应的叶子结点**
